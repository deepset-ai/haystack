---
title: "Amazon Bedrock"
id: integrations-amazon-bedrock
description: "Amazon Bedrock integration for Haystack"
slug: "/integrations-amazon-bedrock"
---


## `haystack_integrations.common.amazon_bedrock.errors`

### `AmazonBedrockError`

Bases: <code>Exception</code>

Any error generated by the Amazon Bedrock integration.

This error wraps its source transparently in such a way that its attributes
can be accessed directly: for example, if the original error has a `message` attribute,
`AmazonBedrockError.message` will exist and have the expected content.

### `AWSConfigurationError`

Bases: <code>AmazonBedrockError</code>

Exception raised when AWS is not configured correctly

### `AmazonBedrockConfigurationError`

Bases: <code>AmazonBedrockError</code>

Exception raised when AmazonBedrock node is not configured correctly

### `AmazonBedrockInferenceError`

Bases: <code>AmazonBedrockError</code>

Exception for issues that occur in the Bedrock inference node

## `haystack_integrations.common.amazon_bedrock.errors`

### `AmazonBedrockError`

Bases: <code>Exception</code>

Any error generated by the Amazon Bedrock integration.

This error wraps its source transparently in such a way that its attributes
can be accessed directly: for example, if the original error has a `message` attribute,
`AmazonBedrockError.message` will exist and have the expected content.

### `AWSConfigurationError`

Bases: <code>AmazonBedrockError</code>

Exception raised when AWS is not configured correctly

### `AmazonBedrockConfigurationError`

Bases: <code>AmazonBedrockError</code>

Exception raised when AmazonBedrock node is not configured correctly

### `AmazonBedrockInferenceError`

Bases: <code>AmazonBedrockError</code>

Exception for issues that occur in the Bedrock inference node

## `haystack_integrations.common.s3.errors`

### `S3Error`

Bases: <code>Exception</code>

Exception for issues that occur in the S3 based components

### `S3ConfigurationError`

Bases: <code>S3Error</code>

Exception raised when AmazonS3 node is not configured correctly

### `S3StorageError`

Bases: <code>S3Error</code>

This exception is raised when an error occurs while interacting with a S3Storage object.

## `haystack_integrations.common.s3.utils`

### `S3Storage`

This class provides a storage class for downloading files from an AWS S3 bucket.

#### `__init__`

```python
__init__(
    s3_bucket: str,
    session: Session,
    s3_prefix: str | None = None,
    endpoint_url: str | None = None,
    config: Config | None = None,
) -> None
```

Initializes the S3Storage object with the provided parameters.

**Parameters:**

- **s3_bucket** (<code>str</code>) – The name of the S3 bucket to download files from.
- **session** (<code>Session</code>) – The session to use for the S3 client.
- **s3_prefix** (<code>str | None</code>) – The optional prefix of the files in the S3 bucket.
  Can be used to specify folder or naming structure.
  For example, if the file is in the folder "folder/subfolder/file.txt",
  the s3_prefix should be "folder/subfolder/". If the file is in the root of the S3 bucket,
  the s3_prefix should be None.
- **endpoint_url** (<code>str | None</code>) – The endpoint URL of the S3 bucket to download files from.
- **config** (<code>Config | None</code>) – The configuration to use for the S3 client.

#### `download`

```python
download(key: str, local_file_path: Path) -> None
```

Download a file from S3.

**Parameters:**

- **key** (<code>str</code>) – The key of the file to download.
- **local_file_path** (<code>Path</code>) – The folder path to download the file to.
  It will be created if it does not exist. The file will be downloaded to
  the folder with the same name as the key.

**Raises:**

- <code>S3ConfigurationError</code> – If the S3 session client cannot be created.
- <code>S3StorageError</code> – If the file does not exist in the S3 bucket
  or the file cannot be downloaded.

#### `from_env`

```python
from_env(*, session: Session, config: Config) -> S3Storage
```

Create a S3Storage object from environment variables.

## `haystack_integrations.components.downloaders.s3.s3_downloader`

### `S3Downloader`

A component for downloading files from AWS S3 Buckets to local filesystem.
Supports filtering by file extensions.

#### `__init__`

```python
__init__(
    *,
    aws_access_key_id: Secret | None = Secret.from_env_var(
        "AWS_ACCESS_KEY_ID", strict=False
    ),
    aws_secret_access_key: Secret | None = Secret.from_env_var(
        "AWS_SECRET_ACCESS_KEY", strict=False
    ),
    aws_session_token: Secret | None = Secret.from_env_var(
        "AWS_SESSION_TOKEN", strict=False
    ),
    aws_region_name: Secret | None = Secret.from_env_var(
        "AWS_DEFAULT_REGION", strict=False
    ),
    aws_profile_name: Secret | None = Secret.from_env_var(
        "AWS_PROFILE", strict=False
    ),
    boto3_config: dict[str, Any] | None = None,
    file_root_path: str | None = None,
    file_extensions: list[str] | None = None,
    file_name_meta_key: str = "file_name",
    max_workers: int = 32,
    max_cache_size: int = 100,
    s3_key_generation_function: Callable[[Document], str] | None = None
) -> None
```

Initializes the `S3Downloader` with the provided parameters.

Note that the AWS credentials are not required if the AWS environment is configured correctly. These are loaded
automatically from the environment or the AWS configuration file and do not need to be provided explicitly via
the constructor. If the AWS environment is not configured users need to provide the AWS credentials via the
constructor. Three required parameters are `aws_access_key_id`, `aws_secret_access_key`,
and `aws_region_name`.

**Parameters:**

- **aws_access_key_id** (<code>Secret | None</code>) – AWS access key ID.
- **aws_secret_access_key** (<code>Secret | None</code>) – AWS secret access key.
- **aws_session_token** (<code>Secret | None</code>) – AWS session token.
- **aws_region_name** (<code>Secret | None</code>) – AWS region name.
- **aws_profile_name** (<code>Secret | None</code>) – AWS profile name.
- **boto3_config** (<code>dict\[str, Any\] | None</code>) – The configuration for the boto3 client.
- **file_root_path** (<code>str | None</code>) – The path where the file will be downloaded.
  Can be set through this parameter or the `FILE_ROOT_PATH` environment variable.
  If none of them is set, a `ValueError` is raised.
- **file_extensions** (<code>list\[str\] | None</code>) – The file extensions that are permitted to be downloaded.
  By default, all file extensions are allowed.
- **max_workers** (<code>int</code>) – The maximum number of workers to use for concurrent downloads.
- **max_cache_size** (<code>int</code>) – The maximum number of files to cache.
- **file_name_meta_key** (<code>str</code>) – The name of the meta key that contains the file name to download. The file name
  will also be used to create local file path for download.
  By default, the `Document.meta["file_name"]` is used. If you want to use a
  different key in `Document.meta`, you can set it here.
- **s3_key_generation_function** (<code>Callable\\[[Document\], str\] | None</code>) – An optional function that generates the S3 key for the file to download.
  If not provided, the default behavior is to use `Document.meta[file_name_meta_key]`.
  The function must accept a `Document` object and return a string.
  If the environment variable `S3_DOWNLOADER_PREFIX` is set, its value will be automatically
  prefixed to the generated S3 key.

**Raises:**

- <code>ValueError</code> – If the `file_root_path` is not set through
  the constructor or the `FILE_ROOT_PATH` environment variable.

#### `warm_up`

```python
warm_up() -> None
```

Warm up the component by initializing the settings and storage.

#### `run`

```python
run(documents: list[Document]) -> dict[str, list[Document]]
```

Download files from AWS S3 Buckets to local filesystem.

Return enriched `Document`s with the path of the downloaded file.

**Parameters:**

- **documents** (<code>list\[Document\]</code>) – Document containing the name of the file to download in the meta field.

**Returns:**

- <code>dict\[str, list\[Document\]\]</code> – A dictionary with:
- `documents`: The downloaded `Document`s; each has `meta['file_path']`.

**Raises:**

- <code>S3Error</code> – If a download attempt fails or the file does not exist in the S3 bucket.
- <code>ValueError</code> – If the path where files will be downloaded is not set.

#### `to_dict`

```python
to_dict() -> dict[str, Any]
```

Serialize the component to a dictionary.

#### `from_dict`

```python
from_dict(data: dict[str, Any]) -> S3Downloader
```

Deserializes the component from a dictionary.

**Parameters:**

- **data** (<code>dict\[str, Any\]</code>) – Dictionary to deserialize from.

**Returns:**

- <code>S3Downloader</code> – Deserialized component.

## `haystack_integrations.components.embedders.amazon_bedrock.document_embedder`

### `AmazonBedrockDocumentEmbedder`

A component for computing Document embeddings using Amazon Bedrock.
The embedding of each Document is stored in the `embedding` field of the Document.

Usage example:

```python
import os
from haystack.dataclasses import Document
from haystack_integrations.components.embedders.amazon_bedrock import AmazonBedrockDocumentEmbedder

os.environ["AWS_ACCESS_KEY_ID"] = "..."
os.environ["AWS_SECRET_ACCESS_KEY_ID"] = "..."
os.environ["AWS_DEFAULT_REGION"] = "..."

embedder = AmazonBedrockDocumentEmbedder(
    model="cohere.embed-english-v3",
    input_type="search_document",
)

doc = Document(content="I love Paris in the winter.", meta={"name": "doc1"})

result = embedder.run([doc])
print(result['documents'][0].embedding)

# [0.002, 0.032, 0.504, ...]
```

#### `__init__`

```python
__init__(
    model: str,
    aws_access_key_id: Secret | None = Secret.from_env_var(
        "AWS_ACCESS_KEY_ID", strict=False
    ),
    aws_secret_access_key: Secret | None = Secret.from_env_var(
        "AWS_SECRET_ACCESS_KEY", strict=False
    ),
    aws_session_token: Secret | None = Secret.from_env_var(
        "AWS_SESSION_TOKEN", strict=False
    ),
    aws_region_name: Secret | None = Secret.from_env_var(
        "AWS_DEFAULT_REGION", strict=False
    ),
    aws_profile_name: Secret | None = Secret.from_env_var(
        "AWS_PROFILE", strict=False
    ),
    batch_size: int = 32,
    progress_bar: bool = True,
    meta_fields_to_embed: list[str] | None = None,
    embedding_separator: str = "\n",
    boto3_config: dict[str, Any] | None = None,
    **kwargs: Any
) -> None
```

Initializes the AmazonBedrockDocumentEmbedder with the provided parameters. The parameters are passed to the
Amazon Bedrock client.

Note that the AWS credentials are not required if the AWS environment is configured correctly. These are loaded
automatically from the environment or the AWS configuration file and do not need to be provided explicitly via
the constructor. If the AWS environment is not configured users need to provide the AWS credentials via the
constructor. Aside from model, three required parameters are `aws_access_key_id`, `aws_secret_access_key`,
and `aws_region_name`.

**Parameters:**

- **model** (<code>str</code>) – The embedding model to use.
  Amazon Titan and Cohere embedding models are supported, for example:
  "amazon.titan-embed-text-v1", "amazon.titan-embed-text-v2:0", "amazon.titan-embed-image-v1",
  "cohere.embed-english-v3", "cohere.embed-multilingual-v3", "cohere.embed-v4:0".
  To find all supported models, refer to the Amazon Bedrock
  [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) and
  filter for "embedding", then select models from the Amazon Titan and Cohere series.
- **aws_access_key_id** (<code>Secret | None</code>) – AWS access key ID.
- **aws_secret_access_key** (<code>Secret | None</code>) – AWS secret access key.
- **aws_session_token** (<code>Secret | None</code>) – AWS session token.
- **aws_region_name** (<code>Secret | None</code>) – AWS region name.
- **aws_profile_name** (<code>Secret | None</code>) – AWS profile name.
- **batch_size** (<code>int</code>) – Number of Documents to encode at once.
  Only Cohere models support batch inference. This parameter is ignored for Amazon Titan models.
- **progress_bar** (<code>bool</code>) – Whether to show a progress bar or not. Can be helpful to disable in production deployments
  to keep the logs clean.
- **meta_fields_to_embed** (<code>list\[str\] | None</code>) – List of meta fields that should be embedded along with the Document text.
- **embedding_separator** (<code>str</code>) – Separator used to concatenate the meta fields to the Document text.
- **boto3_config** (<code>dict\[str, Any\] | None</code>) – The configuration for the boto3 client.
- **kwargs** (<code>Any</code>) – Additional parameters to pass for model inference. For example, `input_type` and `truncate` for
  Cohere models.

**Raises:**

- <code>ValueError</code> – If the model is not supported.
- <code>AmazonBedrockConfigurationError</code> – If the AWS environment is not configured correctly.

#### `run`

```python
run(documents: list[Document]) -> dict[str, list[Document]]
```

Embed the provided `Document`s using the specified model.

**Parameters:**

- **documents** (<code>list\[Document\]</code>) – The `Document`s to embed.

**Returns:**

- <code>dict\[str, list\[Document\]\]</code> – A dictionary with the following keys:
- `documents`: The `Document`s with the `embedding` field populated.

**Raises:**

- <code>AmazonBedrockInferenceError</code> – If the inference fails.

#### `to_dict`

```python
to_dict() -> dict[str, Any]
```

Serializes the component to a dictionary.

**Returns:**

- <code>dict\[str, Any\]</code> – Dictionary with serialized data.

#### `from_dict`

```python
from_dict(data: dict[str, Any]) -> AmazonBedrockDocumentEmbedder
```

Deserializes the component from a dictionary.

**Parameters:**

- **data** (<code>dict\[str, Any\]</code>) – Dictionary to deserialize from.

**Returns:**

- <code>AmazonBedrockDocumentEmbedder</code> – Deserialized component.

## `haystack_integrations.components.embedders.amazon_bedrock.document_image_embedder`

### `AmazonBedrockDocumentImageEmbedder`

A component for computing Document embeddings based on images using Amazon Bedrock models.

The embedding of each Document is stored in the `embedding` field of the Document.

### Usage example

```python
from haystack import Document
rom haystack_integrations.components.embedders.amazon_bedrock import AmazonBedrockDocumentImageEmbedder

os.environ["AWS_ACCESS_KEY_ID"] = "..."
os.environ["AWS_SECRET_ACCESS_KEY_ID"] = "..."
os.environ["AWS_DEFAULT_REGION"] = "..."

embedder = AmazonBedrockDocumentImageEmbedder(model="amazon.titan-embed-image-v1")

documents = [
    Document(content="A photo of a cat", meta={"file_path": "cat.jpg"}),
    Document(content="A photo of a dog", meta={"file_path": "dog.jpg"}),
]

result = embedder.run(documents=documents)
documents_with_embeddings = result["documents"]
print(documents_with_embeddings)

# [Document(id=...,
#           content='A photo of a cat',
#           meta={'file_path': 'cat.jpg',
#                 'embedding_source': {'type': 'image', 'file_path_meta_field': 'file_path'}},
#           embedding=vector of size 512),
#  ...]
```

#### `__init__`

```python
__init__(
    *,
    model: str,
    aws_access_key_id: Secret | None = Secret.from_env_var(
        "AWS_ACCESS_KEY_ID", strict=False
    ),
    aws_secret_access_key: Secret | None = Secret.from_env_var(
        "AWS_SECRET_ACCESS_KEY", strict=False
    ),
    aws_session_token: Secret | None = Secret.from_env_var(
        "AWS_SESSION_TOKEN", strict=False
    ),
    aws_region_name: Secret | None = Secret.from_env_var(
        "AWS_DEFAULT_REGION", strict=False
    ),
    aws_profile_name: Secret | None = Secret.from_env_var(
        "AWS_PROFILE", strict=False
    ),
    file_path_meta_field: str = "file_path",
    root_path: str | None = None,
    image_size: tuple[int, int] | None = None,
    progress_bar: bool = True,
    boto3_config: dict[str, Any] | None = None,
    **kwargs: Any
) -> None
```

Creates a AmazonBedrockDocumentImageEmbedder component.

**Parameters:**

- **model** (<code>str</code>) – The embedding model to use.
  Amazon Titan and Cohere multimodal embedding models are supported, for example:
  "amazon.titan-embed-image-v1", "cohere.embed-english-v3", "cohere.embed-multilingual-v3",
  "cohere.embed-v4:0".
  To find all supported models, refer to the Amazon Bedrock
  [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) and
  filter for "embedding", then select multimodal models from the Amazon Titan and Cohere series.
- **aws_access_key_id** (<code>Secret | None</code>) – AWS access key ID.
- **aws_secret_access_key** (<code>Secret | None</code>) – AWS secret access key.
- **aws_session_token** (<code>Secret | None</code>) – AWS session token.
- **aws_region_name** (<code>Secret | None</code>) – AWS region name.
- **aws_profile_name** (<code>Secret | None</code>) – AWS profile name.
- **file_path_meta_field** (<code>str</code>) – The metadata field in the Document that contains the file path to the image or PDF.
- **root_path** (<code>str | None</code>) – The root directory path where document files are located. If provided, file paths in
  document metadata will be resolved relative to this path. If None, file paths are treated as absolute paths.
- **image_size** (<code>tuple\[int, int\] | None</code>) – If provided, resizes the image to fit within the specified dimensions (width, height) while
  maintaining aspect ratio. This reduces file size, memory usage, and processing time, which is beneficial
  when working with models that have resolution constraints or when transmitting images to remote services.
- **progress_bar** (<code>bool</code>) – If `True`, shows a progress bar when embedding documents.
- **boto3_config** (<code>dict\[str, Any\] | None</code>) – The configuration for the boto3 client.
- **kwargs** (<code>Any</code>) – Additional parameters to pass for model inference.
  For example, `embeddingConfig` for Amazon Titan models and
  `embedding_types` for Cohere models.

**Raises:**

- <code>ValueError</code> – If the model is not supported.
- <code>AmazonBedrockConfigurationError</code> – If the AWS environment is not configured correctly.

#### `to_dict`

```python
to_dict() -> dict[str, Any]
```

Serializes the component to a dictionary.

**Returns:**

- <code>dict\[str, Any\]</code> – Dictionary with serialized data.

#### `from_dict`

```python
from_dict(data: dict[str, Any]) -> AmazonBedrockDocumentImageEmbedder
```

Deserializes the component from a dictionary.

**Parameters:**

- **data** (<code>dict\[str, Any\]</code>) – Dictionary to deserialize from.

**Returns:**

- <code>AmazonBedrockDocumentImageEmbedder</code> – Deserialized component.

#### `run`

```python
run(documents: list[Document]) -> dict[str, list[Document]]
```

Embed a list of images.

**Parameters:**

- **documents** (<code>list\[Document\]</code>) – Documents to embed.

**Returns:**

- <code>dict\[str, list\[Document\]\]</code> – A dictionary with the following keys:
- `documents`: Documents with embeddings.

## `haystack_integrations.components.embedders.amazon_bedrock.text_embedder`

### `AmazonBedrockTextEmbedder`

A component for embedding strings using Amazon Bedrock.

Usage example:

```python
import os
from haystack_integrations.components.embedders.amazon_bedrock import AmazonBedrockTextEmbedder

os.environ["AWS_ACCESS_KEY_ID"] = "..."
os.environ["AWS_SECRET_ACCESS_KEY_ID"] = "..."
os.environ["AWS_DEFAULT_REGION"] = "..."

embedder = AmazonBedrockTextEmbedder(
    model="cohere.embed-english-v3",
    input_type="search_query",
)

print(text_embedder.run("I love Paris in the summer."))

# {'embedding': [0.002, 0.032, 0.504, ...]}
```

#### `__init__`

```python
__init__(
    model: str,
    aws_access_key_id: Secret | None = Secret.from_env_var(
        "AWS_ACCESS_KEY_ID", strict=False
    ),
    aws_secret_access_key: Secret | None = Secret.from_env_var(
        "AWS_SECRET_ACCESS_KEY", strict=False
    ),
    aws_session_token: Secret | None = Secret.from_env_var(
        "AWS_SESSION_TOKEN", strict=False
    ),
    aws_region_name: Secret | None = Secret.from_env_var(
        "AWS_DEFAULT_REGION", strict=False
    ),
    aws_profile_name: Secret | None = Secret.from_env_var(
        "AWS_PROFILE", strict=False
    ),
    boto3_config: dict[str, Any] | None = None,
    **kwargs: Any
) -> None
```

Initializes the AmazonBedrockTextEmbedder with the provided parameters. The parameters are passed to the
Amazon Bedrock client.

Note that the AWS credentials are not required if the AWS environment is configured correctly. These are loaded
automatically from the environment or the AWS configuration file and do not need to be provided explicitly via
the constructor. If the AWS environment is not configured users need to provide the AWS credentials via the
constructor. Aside from model, three required parameters are `aws_access_key_id`, `aws_secret_access_key`,
and `aws_region_name`.

**Parameters:**

- **model** (<code>str</code>) – The embedding model to use.
  Amazon Titan and Cohere embedding models are supported, for example:
  "amazon.titan-embed-text-v1", "amazon.titan-embed-text-v2:0", "amazon.titan-embed-image-v1",
  "cohere.embed-english-v3", "cohere.embed-multilingual-v3", "cohere.embed-v4:0".
  To find all supported models, refer to the Amazon Bedrock
  [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) and
  filter for "embedding", then select models from the Amazon Titan and Cohere series.
- **aws_access_key_id** (<code>Secret | None</code>) – AWS access key ID.
- **aws_secret_access_key** (<code>Secret | None</code>) – AWS secret access key.
- **aws_session_token** (<code>Secret | None</code>) – AWS session token.
- **aws_region_name** (<code>Secret | None</code>) – AWS region name.
- **aws_profile_name** (<code>Secret | None</code>) – AWS profile name.
- **boto3_config** (<code>dict\[str, Any\] | None</code>) – The configuration for the boto3 client.
- **kwargs** (<code>Any</code>) – Additional parameters to pass for model inference. For example, `input_type` and `truncate` for
  Cohere models.

**Raises:**

- <code>ValueError</code> – If the model is not supported.
- <code>AmazonBedrockConfigurationError</code> – If the AWS environment is not configured correctly.

#### `run`

```python
run(text: str) -> dict[str, list[float]]
```

Embeds the input text using the Amazon Bedrock model.

**Parameters:**

- **text** (<code>str</code>) – The input text to embed.

**Returns:**

- <code>dict\[str, list\[float\]\]</code> – A dictionary with the following keys:
- `embedding`: The embedding of the input text.

**Raises:**

- <code>TypeError</code> – If the input text is not a string.
- <code>AmazonBedrockInferenceError</code> – If the model inference fails.

#### `to_dict`

```python
to_dict() -> dict[str, Any]
```

Serializes the component to a dictionary.

**Returns:**

- <code>dict\[str, Any\]</code> – Dictionary with serialized data.

#### `from_dict`

```python
from_dict(data: dict[str, Any]) -> AmazonBedrockTextEmbedder
```

Deserializes the component from a dictionary.

**Parameters:**

- **data** (<code>dict\[str, Any\]</code>) – Dictionary to deserialize from.

**Returns:**

- <code>AmazonBedrockTextEmbedder</code> – Deserialized component.

## `haystack_integrations.components.generators.amazon_bedrock.adapters`

### `BedrockModelAdapter`

Bases: <code>ABC</code>

Base class for Amazon Bedrock model adapters.

Each subclass of this class is designed to address the unique specificities of a particular LLM it adapts,
focusing on preparing the requests and extracting the responses from the Amazon Bedrock hosted LLMs.

**Parameters:**

- **model_kwargs** (<code>dict\[str, Any\]</code>) – Keyword arguments for the model. You can find the full list of parameters in the
  Amazon Bedrock API [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html).
- **max_length** (<code>int | None</code>) – Maximum length of generated text. This is mapped to the correct parameter for each model.
  It will be overridden by the corresponding parameter in the `model_kwargs` if it is present.

#### `prepare_body`

```python
prepare_body(prompt: str, **inference_kwargs: Any) -> dict[str, Any]
```

Prepares the body for the Amazon Bedrock request.
Each subclass should implement this method to prepare the request body for the specific model.

**Parameters:**

- **prompt** (<code>str</code>) – The prompt to be sent to the model.
- **inference_kwargs** (<code>Any</code>) – Additional keyword arguments passed to the handler.

**Returns:**

- <code>dict\[str, Any\]</code> – A dictionary containing the body for the request.

#### `get_responses`

```python
get_responses(response_body: dict[str, Any]) -> list[str]
```

Extracts the responses from the Amazon Bedrock response.

**Parameters:**

- **response_body** (<code>dict\[str, Any\]</code>) – The response body from the Amazon Bedrock request.

**Returns:**

- <code>list\[str\]</code> – A list of responses.

#### `get_stream_responses`

```python
get_stream_responses(
    stream: EventStream, streaming_callback: SyncStreamingCallbackT
) -> list[str]
```

Extracts the responses from the Amazon Bedrock streaming response.

**Parameters:**

- **stream** (<code>EventStream</code>) – The streaming response from the Amazon Bedrock request.
- **streaming_callback** (<code>SyncStreamingCallbackT</code>) – The handler for the streaming response.

**Returns:**

- <code>list\[str\]</code> – A list of string responses.

### `AnthropicClaudeAdapter`

Bases: <code>BedrockModelAdapter</code>

Adapter for the Anthropic Claude models.

**Parameters:**

- **model_kwargs** (<code>dict\[str, Any\]</code>) – Keyword arguments for the model. You can find the full list of parameters in the
  Amazon Bedrock API documentation for the Claude model
  [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-claude.html).
  Some example parameters are:
- use_messages_api: Whether to use the messages API, default: True
- include_thinking: Whether to include thinking output, default: True
- thinking_tag: XML tag for thinking content, default: "thinking"
- **max_length** (<code>int | None</code>) – Maximum length of generated text

#### `prepare_body`

```python
prepare_body(prompt: str, **inference_kwargs: Any) -> dict[str, Any]
```

Prepares the body for the Claude model

**Parameters:**

- **prompt** (<code>str</code>) – The prompt to be sent to the model.
- **inference_kwargs** (<code>Any</code>) – Additional keyword arguments passed to the handler.

**Returns:**

- <code>dict\[str, Any\]</code> – A dictionary with the following keys:
- `prompt`: The prompt to be sent to the model.
- specified inference parameters.

### `MistralAdapter`

Bases: <code>BedrockModelAdapter</code>

Adapter for the Mistral models.

#### `prepare_body`

```python
prepare_body(prompt: str, **inference_kwargs: Any) -> dict[str, Any]
```

Prepares the body for the Mistral model

**Parameters:**

- **prompt** (<code>str</code>) – The prompt to be sent to the model.
- **inference_kwargs** (<code>Any</code>) – Additional keyword arguments passed to the handler.

**Returns:**

- <code>dict\[str, Any\]</code> – A dictionary with the following keys:
- `prompt`: The prompt to be sent to the model.
- specified inference parameters.

### `CohereCommandAdapter`

Bases: <code>BedrockModelAdapter</code>

Adapter for the Cohere Command model.

#### `prepare_body`

```python
prepare_body(prompt: str, **inference_kwargs: Any) -> dict[str, Any]
```

Prepares the body for the Command model

**Parameters:**

- **prompt** (<code>str</code>) – The prompt to be sent to the model.
- **inference_kwargs** (<code>Any</code>) – Additional keyword arguments passed to the handler.

**Returns:**

- <code>dict\[str, Any\]</code> – A dictionary with the following keys:
- `prompt`: The prompt to be sent to the model.
- specified inference parameters.

### `CohereCommandRAdapter`

Bases: <code>BedrockModelAdapter</code>

Adapter for the Cohere Command R models.

#### `prepare_body`

```python
prepare_body(prompt: str, **inference_kwargs: Any) -> dict[str, Any]
```

Prepares the body for the Command model

**Parameters:**

- **prompt** (<code>str</code>) – The prompt to be sent to the model.
- **inference_kwargs** (<code>Any</code>) – Additional keyword arguments passed to the handler.

**Returns:**

- <code>dict\[str, Any\]</code> – A dictionary with the following keys:
- `prompt`: The prompt to be sent to the model.
- specified inference parameters.

### `AI21LabsJurassic2Adapter`

Bases: <code>BedrockModelAdapter</code>

Model adapter for AI21 Labs' Jurassic 2 models.

#### `prepare_body`

```python
prepare_body(prompt: str, **inference_kwargs: Any) -> dict[str, Any]
```

Prepares the body for the Jurassic 2 model.

**Parameters:**

- **prompt** (<code>str</code>) – The prompt to be sent to the model.
- **inference_kwargs** (<code>Any</code>) – Additional keyword arguments passed to the handler.

**Returns:**

- <code>dict\[str, Any\]</code> – A dictionary with the following keys:
- `prompt`: The prompt to be sent to the model.
- specified inference parameters.

### `AmazonTitanAdapter`

Bases: <code>BedrockModelAdapter</code>

Adapter for Amazon's Titan models.

#### `prepare_body`

```python
prepare_body(prompt: str, **inference_kwargs: Any) -> dict[str, Any]
```

Prepares the body for the Titan model

**Parameters:**

- **prompt** (<code>str</code>) – The prompt to be sent to the model.
- **inference_kwargs** (<code>Any</code>) – Additional keyword arguments passed to the handler.

**Returns:**

- <code>dict\[str, Any\]</code> – A dictionary with the following keys
- `inputText`: The prompt to be sent to the model.
- specified inference parameters.

### `MetaLlamaAdapter`

Bases: <code>BedrockModelAdapter</code>

Adapter for Meta's Llama2 models.

#### `prepare_body`

```python
prepare_body(prompt: str, **inference_kwargs: Any) -> dict[str, Any]
```

Prepares the body for the Llama2 model

**Parameters:**

- **prompt** (<code>str</code>) – The prompt to be sent to the model.
- **inference_kwargs** (<code>Any</code>) – Additional keyword arguments passed to the handler.

**Returns:**

- <code>dict\[str, Any\]</code> – A dictionary with the following keys:
- `prompt`: The prompt to be sent to the model.
- specified inference parameters.

## `haystack_integrations.components.generators.amazon_bedrock.chat.chat_generator`

### `AmazonBedrockChatGenerator`

Completes chats using LLMs hosted on Amazon Bedrock available via the Bedrock Converse API.

For example, to use the Anthropic Claude 3 Sonnet model, initialize this component with the
'anthropic.claude-3-5-sonnet-20240620-v1:0' model name.

**Usage example**

```python
from haystack_integrations.components.generators.amazon_bedrock import AmazonBedrockChatGenerator
from haystack.dataclasses import ChatMessage
from haystack.components.generators.utils import print_streaming_chunk

messages = [ChatMessage.from_system("\nYou are a helpful, respectful and honest assistant, answer in German only"),
            ChatMessage.from_user("What's Natural Language Processing?")]


client = AmazonBedrockChatGenerator(model="anthropic.claude-3-5-sonnet-20240620-v1:0",
                                    streaming_callback=print_streaming_chunk)
client.run(messages, generation_kwargs={"max_tokens": 512})
```

**Multimodal example**

```python
from haystack.dataclasses import ChatMessage, ImageContent
from haystack_integrations.components.generators.amazon_bedrock import AmazonBedrockChatGenerator

generator = AmazonBedrockChatGenerator(model="anthropic.claude-3-5-sonnet-20240620-v1:0")

image_content = ImageContent.from_file_path(file_path="apple.jpg")

message = ChatMessage.from_user(content_parts=["Describe the image using 10 words at most.", image_content])

response = generator.run(messages=[message])["replies"][0].text

print(response)
> The image shows a red apple.
```

**Tool usage example**

AmazonBedrockChatGenerator supports Haystack's unified tool architecture, allowing tools to be used
across different chat generators. The same tool definitions and usage patterns work consistently
whether using Amazon Bedrock, OpenAI, Ollama, or any other supported LLM providers.

```python
from haystack.dataclasses import ChatMessage
from haystack.tools import Tool
from haystack_integrations.components.generators.amazon_bedrock import AmazonBedrockChatGenerator

def weather(city: str):
    return f'The weather in {city} is sunny and 32°C'

# Define tool parameters
tool_parameters = {
    "type": "object",
    "properties": {"city": {"type": "string"}},
    "required": ["city"]
}

# Create weather tool
weather_tool = Tool(
    name="weather",
    description="useful to determine the weather in a given location",
    parameters=tool_parameters,
    function=weather
)

# Initialize generator with tool
client = AmazonBedrockChatGenerator(
    model="anthropic.claude-3-5-sonnet-20240620-v1:0",
    tools=[weather_tool]
)

# Run initial query
messages = [ChatMessage.from_user("What's the weather like in Paris?")]
results = client.run(messages=messages)

# Get tool call from response
tool_message = next(msg for msg in results["replies"] if msg.tool_call)
tool_call = tool_message.tool_call

# Execute tool and send result back
weather_result = weather(**tool_call.arguments)
new_messages = [
    messages[0],
    tool_message,
    ChatMessage.from_tool(tool_result=weather_result, origin=tool_call)
]

# Get final response
final_result = client.run(new_messages)
print(final_result["replies"][0].text)

> Based on the information I've received, I can tell you that the weather in Paris is
> currently sunny with a temperature of 32°C (which is about 90°F).
```

**Prompt caching**

This component supports prompt caching. You can use the `tools_cachepoint_config` parameter to configure the cache
point for tools.
To cache messages, you can use the `cachePoint` key in `ChatMessage.meta` attribute.

```python
ChatMessage.from_user("Long message...", meta={"cachePoint": {"type": "default"}})
```

For more information, see the [Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html).

**Authentication**

AmazonBedrockChatGenerator uses AWS for authentication. You can use the AWS CLI to authenticate through your IAM.
For more information on setting up an IAM identity-based policy, see [Amazon Bedrock documentation]
(https://docs.aws.amazon.com/bedrock/latest/userguide/security_iam_id-based-policy-examples.html).

If the AWS environment is configured correctly, the AWS credentials are not required as they're loaded
automatically from the environment or the AWS configuration file.
If the AWS environment is not configured, set `aws_access_key_id`, `aws_secret_access_key`,
and `aws_region_name` as environment variables or pass them as
[Secret](https://docs.haystack.deepset.ai/docs/secret-management) arguments. Make sure the region you set
supports Amazon Bedrock.

#### `__init__`

```python
__init__(
    model: str,
    aws_access_key_id: Secret | None = Secret.from_env_var(
        ["AWS_ACCESS_KEY_ID"], strict=False
    ),
    aws_secret_access_key: Secret | None = Secret.from_env_var(
        ["AWS_SECRET_ACCESS_KEY"], strict=False
    ),
    aws_session_token: Secret | None = Secret.from_env_var(
        ["AWS_SESSION_TOKEN"], strict=False
    ),
    aws_region_name: Secret | None = Secret.from_env_var(
        ["AWS_DEFAULT_REGION"], strict=False
    ),
    aws_profile_name: Secret | None = Secret.from_env_var(
        ["AWS_PROFILE"], strict=False
    ),
    generation_kwargs: dict[str, Any] | None = None,
    streaming_callback: StreamingCallbackT | None = None,
    boto3_config: dict[str, Any] | None = None,
    tools: ToolsType | None = None,
    *,
    guardrail_config: dict[str, str] | None = None,
    tools_cachepoint_config: dict[str, str] | None = None
) -> None
```

Initializes the `AmazonBedrockChatGenerator` with the provided parameters. The parameters are passed to the
Amazon Bedrock client.

Note that the AWS credentials are not required if the AWS environment is configured correctly. These are loaded
automatically from the environment or the AWS configuration file and do not need to be provided explicitly via
the constructor. If the AWS environment is not configured users need to provide the AWS credentials via the
constructor. Aside from model, three required parameters are `aws_access_key_id`, `aws_secret_access_key`,
and `aws_region_name`.

**Parameters:**

- **model** (<code>str</code>) – The model to use for text generation. The model must be available in Amazon Bedrock and must
  be specified in the format outlined in the [Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html).
- **aws_access_key_id** (<code>Secret | None</code>) – AWS access key ID.
- **aws_secret_access_key** (<code>Secret | None</code>) – AWS secret access key.
- **aws_session_token** (<code>Secret | None</code>) – AWS session token.
- **aws_region_name** (<code>Secret | None</code>) – AWS region name. Make sure the region you set supports Amazon Bedrock.
- **aws_profile_name** (<code>Secret | None</code>) – AWS profile name.
- **generation_kwargs** (<code>dict\[str, Any\] | None</code>) – Keyword arguments sent to the model. These parameters are specific to a model.
  You can find the model specific arguments in the AWS Bedrock API
  [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html).
- **streaming_callback** (<code>StreamingCallbackT | None</code>) – A callback function called when a new token is received from the stream.
  By default, the model is not set up for streaming. To enable streaming, set this parameter to a callback
  function that handles the streaming chunks. The callback function receives a
  [StreamingChunk](https://docs.haystack.deepset.ai/docs/data-classes#streamingchunk) object and switches
  the streaming mode on.
- **boto3_config** (<code>dict\[str, Any\] | None</code>) – The configuration for the boto3 client.
- **tools** (<code>ToolsType | None</code>) – A list of Tool and/or Toolset objects, or a single Toolset for which the model can prepare calls.
  Each tool should have a unique name.
- **guardrail_config** (<code>dict\[str, str\] | None</code>) – Optional configuration for a guardrail that has been created in Amazon Bedrock.
  This must be provided as a dictionary matching either
  [GuardrailConfiguration](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_GuardrailConfiguration.html).
  or, in streaming mode (when `streaming_callback` is set),
  [GuardrailStreamConfiguration](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_GuardrailStreamConfiguration.html).
  If `trace` is set to `enabled`, the guardrail trace will be included under the `trace` key in the `meta`
  attribute of the resulting `ChatMessage`.
  Note: Enabling guardrails in streaming mode may introduce additional latency.
  To manage this, you can adjust the `streamProcessingMode` parameter.
  See the
  [Guardrails Streaming documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-streaming.html)
  for more information.
- **tools_cachepoint_config** (<code>dict\[str, str\] | None</code>) – Optional configuration to use prompt caching for tools.
  The dictionary must match the
  [CachePointBlock schema](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_CachePointBlock.html).
  Example: `{"type": "default", "ttl": "5m"}`

**Raises:**

- <code>ValueError</code> – If the model name is empty or None.
- <code>AmazonBedrockConfigurationError</code> – If the AWS environment is not configured correctly or the model is
  not supported.

#### `to_dict`

```python
to_dict() -> dict[str, Any]
```

Serializes the component to a dictionary.

**Returns:**

- <code>dict\[str, Any\]</code> – Dictionary with serialized data.

#### `from_dict`

```python
from_dict(data: dict[str, Any]) -> AmazonBedrockChatGenerator
```

Deserializes the component from a dictionary.

**Parameters:**

- **data** (<code>dict\[str, Any\]</code>) – Dictionary with serialized data.

**Returns:**

- <code>AmazonBedrockChatGenerator</code> – Instance of `AmazonBedrockChatGenerator`.

#### `run`

```python
run(
    messages: list[ChatMessage],
    streaming_callback: StreamingCallbackT | None = None,
    generation_kwargs: dict[str, Any] | None = None,
    tools: ToolsType | None = None,
) -> dict[str, list[ChatMessage]]
```

Executes a synchronous inference call to the Amazon Bedrock model using the Converse API.

Supports both standard and streaming responses depending on whether a streaming callback is provided.

**Parameters:**

- **messages** (<code>list\[ChatMessage\]</code>) – A list of `ChatMessage` objects forming the chat history.
- **streaming_callback** (<code>StreamingCallbackT | None</code>) – Optional callback for handling streaming outputs.
- **generation_kwargs** (<code>dict\[str, Any\] | None</code>) – Optional dictionary of generation parameters. Some common parameters are:
- `maxTokens`: Maximum number of tokens to generate.
- `stopSequences`: List of stop sequences to stop generation.
- `temperature`: Sampling temperature.
- `topP`: Nucleus sampling parameter.
- **tools** (<code>ToolsType | None</code>) – A list of Tool and/or Toolset objects, or a single Toolset for which the model can prepare calls.
  Each tool should have a unique name.

**Returns:**

- <code>dict\[str, list\[ChatMessage\]\]</code> – A dictionary containing the model-generated replies under the `"replies"` key.

**Raises:**

- <code>AmazonBedrockInferenceError</code> – If the Bedrock inference API call fails.

#### `run_async`

```python
run_async(
    messages: list[ChatMessage],
    streaming_callback: StreamingCallbackT | None = None,
    generation_kwargs: dict[str, Any] | None = None,
    tools: ToolsType | None = None,
) -> dict[str, list[ChatMessage]]
```

Executes an asynchronous inference call to the Amazon Bedrock model using the Converse API.

Designed for use cases where non-blocking or concurrent execution is desired.

**Parameters:**

- **messages** (<code>list\[ChatMessage\]</code>) – A list of `ChatMessage` objects forming the chat history.
- **streaming_callback** (<code>StreamingCallbackT | None</code>) – Optional async-compatible callback for handling streaming outputs.
- **generation_kwargs** (<code>dict\[str, Any\] | None</code>) – Optional dictionary of generation parameters. Some common parameters are:
- `maxTokens`: Maximum number of tokens to generate.
- `stopSequences`: List of stop sequences to stop generation.
- `temperature`: Sampling temperature.
- `topP`: Nucleus sampling parameter.
- **tools** (<code>ToolsType | None</code>) – A list of Tool and/or Toolset objects, or a single Toolset for which the model can prepare calls.
  Each tool should have a unique name.

**Returns:**

- <code>dict\[str, list\[ChatMessage\]\]</code> – A dictionary containing the model-generated replies under the `"replies"` key.

**Raises:**

- <code>AmazonBedrockInferenceError</code> – If the Bedrock inference API call fails.

## `haystack_integrations.components.generators.amazon_bedrock.generator`

### `AmazonBedrockGenerator`

Generates text using models hosted on Amazon Bedrock.

For example, to use the Anthropic Claude model, pass 'anthropic.claude-v2' in the `model` parameter.
Provide AWS credentials either through the local AWS profile or directly through
`aws_access_key_id`, `aws_secret_access_key`, `aws_session_token`, and `aws_region_name` parameters.

### Usage example

```python
from haystack_integrations.components.generators.amazon_bedrock import AmazonBedrockGenerator

generator = AmazonBedrockGenerator(
        model="anthropic.claude-v2",
        max_length=99
)

print(generator.run("Who is the best American actor?"))
```

AmazonBedrockGenerator uses AWS for authentication. You can use the AWS CLI to authenticate through your IAM.
For more information on setting up an IAM identity-based policy, see [Amazon Bedrock documentation]
(https://docs.aws.amazon.com/bedrock/latest/userguide/security_iam_id-based-policy-examples.html).
If the AWS environment is configured correctly, the AWS credentials are not required as they're loaded
automatically from the environment or the AWS configuration file.
If the AWS environment is not configured, set `aws_access_key_id`, `aws_secret_access_key`,
`aws_session_token`, and `aws_region_name` as environment variables or pass them as
[Secret](https://docs.haystack.deepset.ai/docs/secret-management) arguments. Make sure the region you set
supports Amazon Bedrock.

#### `__init__`

```python
__init__(
    model: str,
    aws_access_key_id: Secret | None = Secret.from_env_var(
        "AWS_ACCESS_KEY_ID", strict=False
    ),
    aws_secret_access_key: Secret | None = Secret.from_env_var(
        "AWS_SECRET_ACCESS_KEY", strict=False
    ),
    aws_session_token: Secret | None = Secret.from_env_var(
        "AWS_SESSION_TOKEN", strict=False
    ),
    aws_region_name: Secret | None = Secret.from_env_var(
        "AWS_DEFAULT_REGION", strict=False
    ),
    aws_profile_name: Secret | None = Secret.from_env_var(
        "AWS_PROFILE", strict=False
    ),
    max_length: int | None = None,
    truncate: bool | None = None,
    streaming_callback: Callable[[StreamingChunk], None] | None = None,
    boto3_config: dict[str, Any] | None = None,
    model_family: MODEL_FAMILIES | None = None,
    **kwargs: Any
) -> None
```

Create a new `AmazonBedrockGenerator` instance.

**Parameters:**

- **model** (<code>str</code>) – The name of the model to use.
- **aws_access_key_id** (<code>Secret | None</code>) – The AWS access key ID.
- **aws_secret_access_key** (<code>Secret | None</code>) – The AWS secret access key.
- **aws_session_token** (<code>Secret | None</code>) – The AWS session token.
- **aws_region_name** (<code>Secret | None</code>) – The AWS region name. Make sure the region you set supports Amazon Bedrock.
- **aws_profile_name** (<code>Secret | None</code>) – The AWS profile name.
- **max_length** (<code>int | None</code>) – The maximum length of the generated text. This can also be set in the `kwargs` parameter
  by using the model specific parameter name.
- **truncate** (<code>bool | None</code>) – Deprecated. This parameter no longer has any effect.
- **streaming_callback** (<code>Callable\\[[StreamingChunk\], None\] | None</code>) – A callback function that is called when a new token is received from the stream.
  The callback function accepts StreamingChunk as an argument.
- **boto3_config** (<code>dict\[str, Any\] | None</code>) – The configuration for the boto3 client.
- **model_family** (<code>MODEL_FAMILIES | None</code>) – The model family to use. If not provided, the model adapter is selected based on the model
  name.
- **kwargs** (<code>Any</code>) – Additional keyword arguments to be passed to the model.
  You can find the model specific arguments in AWS Bedrock's
  [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html).
  These arguments are specific to the model. You can find them in the model's documentation.

**Raises:**

- <code>ValueError</code> – If the model name is empty or None.
- <code>AmazonBedrockConfigurationError</code> – If the AWS environment is not configured correctly or the model is
  not supported.

#### `run`

```python
run(
    prompt: str,
    streaming_callback: Callable[[StreamingChunk], None] | None = None,
    generation_kwargs: dict[str, Any] | None = None,
) -> dict[str, list[str] | dict[str, Any]]
```

Generates a list of string response to the given prompt.

**Parameters:**

- **prompt** (<code>str</code>) – The prompt to generate a response for.
- **streaming_callback** (<code>Callable\\[[StreamingChunk\], None\] | None</code>) – A callback function that is called when a new token is received from the stream.
- **generation_kwargs** (<code>dict\[str, Any\] | None</code>) – Additional keyword arguments passed to the generator.

**Returns:**

- <code>dict\[str, list\[str\] | dict\[str, Any\]\]</code> – A dictionary with the following keys:
- `replies`: A list of generated responses.
- `meta`: A dictionary containing response metadata.

**Raises:**

- <code>ValueError</code> – If the prompt is empty or None.
- <code>AmazonBedrockInferenceError</code> – If the model cannot be invoked.

#### `get_model_adapter`

```python
get_model_adapter(
    model: str, model_family: str | None = None
) -> type[BedrockModelAdapter]
```

Gets the model adapter for the given model.

If `model_family` is provided, the adapter for the model family is returned.
If `model_family` is not provided, the adapter is auto-detected based on the model name.

**Parameters:**

- **model** (<code>str</code>) – The model name.
- **model_family** (<code>str | None</code>) – The model family.

**Returns:**

- <code>type\[BedrockModelAdapter\]</code> – The model adapter class, or None if no adapter is found.

**Raises:**

- <code>AmazonBedrockConfigurationError</code> – If the model family is not supported or the model cannot be
  auto-detected.

#### `to_dict`

```python
to_dict() -> dict[str, Any]
```

Serializes the component to a dictionary.

**Returns:**

- <code>dict\[str, Any\]</code> – Dictionary with serialized data.

#### `from_dict`

```python
from_dict(data: dict[str, Any]) -> AmazonBedrockGenerator
```

Deserializes the component from a dictionary.

**Parameters:**

- **data** (<code>dict\[str, Any\]</code>) – Dictionary to deserialize from.

**Returns:**

- <code>AmazonBedrockGenerator</code> – Deserialized component.

## `haystack_integrations.components.rankers.amazon_bedrock.ranker`

### `AmazonBedrockRanker`

Ranks Documents based on their similarity to the query using Amazon Bedrock's Cohere Rerank model.

Documents are indexed from most to least semantically relevant to the query.

Supported Amazon Bedrock models:

- cohere.rerank-v3-5:0
- amazon.rerank-v1:0

Usage example:

```python
from haystack import Document
from haystack.utils import Secret
from haystack_integrations.components.rankers.amazon_bedrock import AmazonBedrockRanker

ranker = AmazonBedrockRanker(
    model="cohere.rerank-v3-5:0",
    top_k=2,
    aws_region_name=Secret.from_token("eu-central-1")
)

docs = [Document(content="Paris"), Document(content="Berlin")]
query = "What is the capital of germany?"
output = ranker.run(query=query, documents=docs)
docs = output["documents"]
```

AmazonBedrockRanker uses AWS for authentication. You can use the AWS CLI to authenticate through your IAM.
For more information on setting up an IAM identity-based policy, see [Amazon Bedrock documentation]
(https://docs.aws.amazon.com/bedrock/latest/userguide/security_iam_id-based-policy-examples.html).

If the AWS environment is configured correctly, the AWS credentials are not required as they're loaded
automatically from the environment or the AWS configuration file.
If the AWS environment is not configured, set `aws_access_key_id`, `aws_secret_access_key`,
and `aws_region_name` as environment variables or pass them as
[Secret](https://docs.haystack.deepset.ai/docs/secret-management) arguments. Make sure the region you set
supports Amazon Bedrock.

#### `to_dict`

```python
to_dict() -> dict[str, Any]
```

Serializes the component to a dictionary.

**Returns:**

- <code>dict\[str, Any\]</code> – Dictionary with serialized data.

#### `from_dict`

```python
from_dict(data: dict[str, Any]) -> AmazonBedrockRanker
```

Deserializes the component from a dictionary.

**Parameters:**

- **data** (<code>dict\[str, Any\]</code>) – The dictionary to deserialize from.

**Returns:**

- <code>AmazonBedrockRanker</code> – The deserialized component.

#### `run`

```python
run(
    query: str, documents: list[Document], top_k: int | None = None
) -> dict[str, list[Document]]
```

Use the Amazon Bedrock Reranker to re-rank the list of documents based on the query.

**Parameters:**

- **query** (<code>str</code>) – Query string.
- **documents** (<code>list\[Document\]</code>) – List of Documents.
- **top_k** (<code>int | None</code>) – The maximum number of Documents you want the Ranker to return.

**Returns:**

- <code>dict\[str, list\[Document\]\]</code> – A dictionary with the following keys:
- `documents`: List of Documents most similar to the given query in descending order of similarity.

**Raises:**

- <code>ValueError</code> – If `top_k` is not > 0.
