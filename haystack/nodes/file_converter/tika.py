# pylint: disable=missing-timeout

from typing import List, Optional, Dict

import time
import logging
from pathlib import Path
import subprocess
from html.parser import HTMLParser

import requests
from tika import parser as tikaparser

from haystack.nodes.file_converter.base import BaseConverter
from haystack.schema import Document


logger = logging.getLogger(__name__)
TIKA_CONTAINER_NAME = "tika"


def launch_tika(sleep=15, delete_existing=False):
    # Start a Tika server via Docker

    logger.debug("Starting Tika ...")
    # This line is needed since it is not possible to start a new docker container with the name tika if there is a stopped image with the same name
    # docker rm only succeeds if the container is stopped, not if it is running
    if delete_existing:
        _ = subprocess.run([f"docker rm --force {TIKA_CONTAINER_NAME}"], shell=True, stdout=subprocess.DEVNULL)
    status = subprocess.run(
        [
            f"docker start {TIKA_CONTAINER_NAME} > /dev/null 2>&1 || docker run -p 9998:9998  --name {TIKA_CONTAINER_NAME} apache/tika:1.28.4"
        ],
        shell=True,
    )
    if status.returncode:
        logger.warning(
            "Tried to start Tika through Docker but this failed. "
            "It is likely that there is already an existing Tika instance running. "
        )
    else:
        time.sleep(sleep)


class TikaXHTMLParser(HTMLParser):
    # Use the built-in HTML parser with minimum dependencies
    def __init__(self):
        self.ingest = True
        self.page = ""
        self.pages: List[str] = []
        super(TikaXHTMLParser, self).__init__()

    def handle_starttag(self, tag, attrs):
        # find page div
        pagediv = [value for attr, value in attrs if attr == "class" and value == "page"]
        if tag == "div" and pagediv:
            self.ingest = True

    def handle_endtag(self, tag):
        # close page div, or a single page without page div, save page and open a new page
        if (tag == "div" or tag == "body") and self.ingest:
            self.ingest = False
            # restore words hyphened to the next line
            self.pages.append(self.page.replace("-\n", ""))
            self.page = ""

    def handle_data(self, data):
        if self.ingest:
            self.page += data


class TikaConverter(BaseConverter):
    def __init__(
        self,
        tika_url: str = "http://localhost:9998/tika",
        valid_languages: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
    ):
        """
        :param tika_url: URL of the Tika server
        :param valid_languages: validate languages from a list of languages specified in the ISO 639-1
                                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                                This option can be used to add test for encoding errors. If the extracted text is
                                not one of the valid languages, then it might likely be encoding error resulting
                                in garbled text.
        :param id_hash_keys: Generate the document id from a custom list of strings that refer to the document's
            attributes. If you want to ensure you don't have duplicate documents in your DocumentStore but texts are
            not unique, you can modify the metadata and pass e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]).
            In this case the id will be generated by using the content and the defined metadata.
        """
        super().__init__(valid_languages=valid_languages, id_hash_keys=id_hash_keys)

        ping = requests.get(tika_url)
        if ping.status_code != 200:
            raise Exception(
                f"Apache Tika server is not reachable at the URL '{tika_url}'. To run it locally"
                f"with Docker, execute: 'docker run -p 9998:9998 apache/tika:1.28.4'"
            )
        self.tika_url = tika_url

    def convert(
        self,
        file_path: Path,
        meta: Optional[Dict[str, str]] = None,
        valid_languages: Optional[List[str]] = None,
        encoding: Optional[str] = None,
        xmlContent: Optional[bool] = True,
        id_hash_keys: Optional[List[str]] = None,
    ) -> List[Document]:
        """
        :param file_path: path of the file to convert
        :param meta: dictionary of meta data key-value pairs to append in the returned document.
        :param valid_languages: validate languages from a list of languages specified in the ISO 639-1
                                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                                This option can be used to add test for encoding errors. If the extracted text is
                                not one of the valid languages, then it might likely be encoding error resulting
                                in garbled text.
        :param encoding: Not applicable
        :param id_hash_keys: Generate the document id from a custom list of strings that refer to the document's
            attributes. If you want to ensure you don't have duplicate documents in your DocumentStore but texts are
            not unique, you can modify the metadata and pass e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]).
            In this case the id will be generated by using the content and the defined metadata.
        :param xmlContent: If True, the content is returned as XML. If False, the content is returned as plain text. Tika's default is False, but xml contains more data that can be used by TikaXHTMLParser for finding page and paragraph breaks
        :return: A list of pages and the extracted meta data of the file.
        """
        if valid_languages is None:
            valid_languages = self.valid_languages
        if id_hash_keys is None:
            id_hash_keys = self.id_hash_keys

        parsed = tikaparser.from_file(file_path.as_posix(), self.tika_url, xmlContent=xmlContent)
        parser = TikaXHTMLParser()
        parser.feed(parsed["content"])

        cleaned_pages = []
        # TODO investigate title of document appearing in the first extracted page
        for page in parser.pages:
            lines = page.splitlines()
            page = "\n".join(lines)
            cleaned_pages.append(page)

        if valid_languages:
            document_text = "".join(cleaned_pages)
            if not self.validate_language(document_text, valid_languages):
                logger.warning(
                    f"The language for {file_path} is not one of {valid_languages}. The file may not have "
                    f"been decoded in the correct text format."
                )

        text = "\f".join(cleaned_pages)
        document = Document(content=text, meta={**parsed["metadata"], **(meta or {})}, id_hash_keys=id_hash_keys)
        return [document]
