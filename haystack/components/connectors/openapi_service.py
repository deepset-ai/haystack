# SPDX-FileCopyrightText: 2022-present deepset GmbH <info@deepset.ai>
#
# SPDX-License-Identifier: Apache-2.0

import json
from typing import Any, Dict, List, Optional, Union

from haystack import component, logging
from haystack.dataclasses import ChatMessage, ChatRole
from haystack.lazy_imports import LazyImport

logger = logging.getLogger(__name__)

with LazyImport("Run 'pip install openapi-service-client'") as openapi_imports:
    from openapi_service_client import ClientConfigurationBuilder, OpenAPIServiceClient
    from openapi_service_client.providers import AnthropicLLMProvider, CohereLLMProvider, OpenAILLMProvider


@component
class OpenAPIServiceConnector:
    """
    The `OpenAPIServiceConnector` component connects the Haystack framework to OpenAPI services.

    It integrates with `ChatMessage` dataclass, where the payload in messages is used to determine the method to be
    called and the parameters to be passed. The response from the service is returned as a `ChatMessage`.

    Function calling payloads from OpenAI, Anthropic, and Cohere LLMs are supported.

    Before using this component, users usually resolve function calling function definitions with a help of
    `OpenAPIServiceToFunctions` component.

    The example below demonstrates how to use the `OpenAPIServiceConnector` to invoke a method on a https://serper.dev/
    service specified via OpenAPI specification.

    Note, however, that `OpenAPIServiceConnector` is usually not meant to be used directly, but rather as part of a
    pipeline that includes the `OpenAPIServiceToFunctions` component and an `OpenAIChatGenerator` component using LLM
    with the function calling capabilities. In the example below we use the function calling payload directly, but in a
    real-world scenario, the function calling payload would usually be generated by the `OpenAIChatGenerator` component.

    Usage example:

    ```python
    import json
    import requests

    from haystack.components.connectors import OpenAPIServiceConnector
    from haystack.dataclasses import ChatMessage


    fc_payload = [{'function': {'arguments': '{"q": "Why was Sam Altman ousted from OpenAI?"}', 'name': 'search'},
                   'id': 'call_PmEBYvZ7mGrQP5PUASA5m9wO', 'type': 'function'}]

    serper_token = <your_serper_dev_token>
    serperdev_openapi_spec = json.loads(requests.get("https://bit.ly/serper_dev_spec").text)
    service_connector = OpenAPIServiceConnector()
    result = service_connector.run(messages=[ChatMessage.from_assistant(json.dumps(fc_payload))],
                                   service_openapi_spec=serperdev_openapi_spec, service_credentials=serper_token)
    print(result)

    >> {'service_response': [ChatMessage(content='{"searchParameters": {"q": "Why was Sam Altman ousted from OpenAI?",
    >> "type": "search", "engine": "google"}, "answerBox": {"snippet": "Concerns over AI safety and OpenAI\'s role
    >> in protecting were at the center of Altman\'s brief ouster from the company."...
    ```

    """

    def __init__(self, provider_map: Optional[Dict[str, Any]] = None, default_provider: Optional[str] = "openai"):
        """
        Initializes the OpenAPIServiceConnector instance

        :param provider_map: A dictionary mapping provider names to their respective LLMProvider instances. The default
        providers are OpenAILLMProvider, AnthropicLLMProvider, and CohereLLMProvider.
        """
        openapi_imports.check()
        self.provider_map = provider_map or {
            "openai": OpenAILLMProvider(),
            "anthropic": AnthropicLLMProvider(),
            "cohere": CohereLLMProvider(),
        }
        if default_provider not in self.provider_map:
            raise ValueError(f"Default provider {default_provider} not found in provider map.")
        self.default_provider = default_provider

    @component.output_types(service_response=Dict[str, Any])
    def run(
        self,
        messages: List[ChatMessage],
        service_openapi_spec: Dict[str, Any],
        service_credentials: Optional[Union[dict, str]] = None,
        llm_provider: Optional[str] = "openai",
    ) -> Dict[str, List[ChatMessage]]:
        """
        Processes a list of chat messages to invoke a method on an OpenAPI service.

        It parses the last message in the list, expecting it to contain an OpenAI function calling descriptor
        (name & parameters) in JSON format.

        :param messages: A list of `ChatMessage` objects containing the messages to be processed. The last message
        should contain the function invocation payload in OpenAI function calling format. See the example in the class
        docstring for the expected format.
        :param service_openapi_spec: The OpenAPI JSON specification object of the service to be invoked.
        :param service_credentials: The credentials to be used for authentication with the service.
        Currently, only the http and apiKey OpenAPI security schemes are supported.
        :param llm_provider: The name of the LLM provider that generated the function calling payload.
        Default is "openai".

        :return: A dictionary with the following keys:
            - `service_response`:  a list of `ChatMessage` objects, each containing the response from the service. The
                                   response is in JSON format, and the `content` attribute of the `ChatMessage` contains
                                   the JSON string.

        :raises ValueError: If the last message is not from the assistant or if it does not contain the correct payload
        to invoke a method on the service.
        """

        last_message = messages[-1]
        if not last_message.is_from(ChatRole.ASSISTANT):
            raise ValueError(f"{last_message} is not from the assistant.")
        if not last_message.content:
            raise ValueError("Function calling message content is empty.")

        llm_provider = self.provider_map.get(llm_provider, self.provider_map[self.default_provider])
        logger.debug(f"Using LLM provider: {llm_provider.__class__.__name__}")

        builder = ClientConfigurationBuilder()
        config_openapi = (
            builder.with_openapi_spec(service_openapi_spec)
            .with_credentials(service_credentials)
            .with_provider(llm_provider)
            .build()
        )
        logger.debug(f"Invoking service {config_openapi.get_openapi_spec().get_name()} with {last_message.content}")
        openapi_service = OpenAPIServiceClient(config_openapi)
        try:
            payload = (
                json.loads(last_message.content) if isinstance(last_message.content, str) else last_message.content
            )
            service_response = openapi_service.invoke(payload)
        except Exception as e:
            logger.error(f"Error invoking OpenAPI endpoint. Error: {e}")
            service_response = {"error": str(e)}
        response_messages = [ChatMessage.from_user(json.dumps(service_response))]

        return {"service_response": response_messages}
