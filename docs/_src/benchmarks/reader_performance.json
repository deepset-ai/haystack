{
    "chart_type": "BarChart",
    "title": "Reader Performance",
    "subtitle": "Time and Accuracy Benchmarks",
    "description": "Here are the performance benchmarks of different Readers that can be used off the shelf in Haystack. Some models are geared towards speed, while others are more performance-focused. Accuracy is measured as F1 and speed as passages/sec.  \n\nEach Reader is benchmarked using the SQuAD v2.0 development set, which contains 11866 question answer pairs. When tokenized using the BERT tokenizer and split using a sliding window approach, these become 12350 passages that are passed into the model. We set max_seq_len=384 and doc_stride=128.\n\nThese benchmarking tests are run using an AWS' P3.2xlarge instance with NVidia V100 GPU. The script used to run them can be found in `haystack/test/benchmarks/reader.py`. \n\nPlease note that we are using the FARMReader class rather than the TransformersReader class. Benchmarking for the latter is coming soon. Also, the F1 measure that is reported here is in fact calculated on token level, rather than word level as is done in the official SQuAD script.",
    "bars": "horizontal",
    "columns": [
        "Model", 
        "F1",
        "Speed"
    ],
    "data": [
        {"F1": 80.67985794671885, "Model": "RoBERTa", "Speed": 92.3039712094936},
        {"F1": 78.23306265318686, "Model": "MiniLM", "Speed": 98.62387044489223},
        {"F1": 74.90271600053505, "Model": "BERT base", "Speed": 99.92750782409666},
        {"F1": 82.64545708097472, "Model": "BERT large", "Speed": 39.529824033964466},
        {"F1": 85.26275190954586, "Model": "XLM-RoBERTa", "Speed": 39.29142006004379}
    ]
}