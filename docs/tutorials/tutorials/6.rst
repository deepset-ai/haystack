Better retrieval via “Dense Passage Retrieval”
==============================================

Importance of Retrievers
~~~~~~~~~~~~~~~~~~~~~~~~

The Retriever has a huge impact on the performance of our overall search
pipeline.

Different types of Retrievers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Sparse
^^^^^^

Family of algorithms based on counting the occurences of words
(bag-of-words) resulting in very sparse vectors with length = vocab
size.

| Examples: BM25, TF-IDF
| Pros: Simple, fast, well explainable
| Cons: Relies on exact keyword matches between query and text

Dense
^^^^^

These retrievers use neural network models to create “dense” embedding
vectors. Within this family there are two different approaches:

a) Single encoder: Use a **single model** to embed both query and
   passage.
b) Dual-encoder: Use **two models**, one to embed the query and one to
   embed the passage

Recent work suggests that dual encoders work better, likely because they
can deal better with the different nature of query and passage (length,
style, syntax …).

Examples: REALM, DPR, Sentence-Transformers … Pros: Captures semantinc
similarity instead of “word matches” (e.g. synonyms, related topics …)
Cons: Computationally more heavy, initial training of model

“Dense Passage Retrieval”
~~~~~~~~~~~~~~~~~~~~~~~~~

In this Tutorial, we want to highlight one “Dense Dual-Encoder” called
Dense Passage Retriever. It was introdoced by Karpukhin et al. (2020,
https://arxiv.org/abs/2004.04906.

Original Abstract:

*“Open-domain question answering relies on efficient passage retrieval
to select candidate contexts, where traditional sparse vector space
models, such as TF-IDF or BM25, are the de facto method. In this work,
we show that retrieval can be practically implemented using dense
representations alone, where embeddings are learned from a small number
of questions and passages by a simple dual-encoder framework. When
evaluated on a wide range of open-domain QA datasets, our dense
retriever outperforms a strong Lucene-BM25 system largely by 9%-19%
absolute in terms of top-20 passage retrieval accuracy, and helps our
end-to-end QA system establish new state-of-the-art on multiple
open-domain QA benchmarks.”*

| Paper: https://arxiv.org/abs/2004.04906
| Original Code: https://fburl.com/qa-dpr

*Use
this*\ `link <https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb>`__\ *to
open the notebook in Google Colab.*

Prepare environment
-------------------

Colab: Enable the GPU runtime
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

| Make sure you enable the GPU runtime to experience decent speed in
  this tutorial.
| **Runtime -> Change Runtime type -> Hardware accelerator -> GPU**

.. code:: ipython3

    # Make sure you have a GPU running
    !nvidia-smi

.. code:: ipython3

    ! pip install git+https://github.com/deepset-ai/haystack.git

.. code:: ipython3

    from haystack import Finder
    from haystack.indexing.cleaning import clean_wiki_text
    from haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http
    from haystack.reader.farm import FARMReader
    from haystack.reader.transformers import TransformersReader
    from haystack.utils import print_answers

Document Store
--------------

Start an Elasticsearch server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can start Elasticsearch on your local machine instance using Docker.
If Docker is not readily available in your environment (e.g. in Colab
notebooks), you can also manually download and execute Elasticsearch
from source.

.. code:: ipython3

    # Recommended: Start Elasticsearch using Docker
    #! docker run -d -p 9200:9200 -e "discovery.type=single-node" elasticsearch:7.6.2
    # wait until ES has started
    #! sleep 30


.. parsed-literal::

    4cc342fd2f0096e6335390e66029716ef452e2853ddd11a5b9802a4fdde20cdc


.. code:: ipython3

    # In Colab / No Docker environments: Start Elasticsearch from source
    ! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q
    ! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz
    ! chown -R daemon:daemon elasticsearch-7.6.2
    
    import os
    from subprocess import Popen, PIPE, STDOUT
    es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],
                       stdout=PIPE, stderr=STDOUT,
                       preexec_fn=lambda: os.setuid(1)  # as daemon
                      )
    # wait until ES has started
    ! sleep 30

.. code:: ipython3

    # Connect to Elasticsearch
    from haystack.database.elasticsearch import ElasticsearchDocumentStore
    
    # We need to set `embedding_field` and `embedding_dim`, when we plan to use a dense retriever
    document_store = ElasticsearchDocumentStore(host="localhost", username="", password="", index="document", 
                                                embedding_field="embedding", embedding_dim=768)


.. parsed-literal::

    07/03/2020 11:46:26 - INFO - elasticsearch -   PUT http://localhost:9200/document [status:200 request:0.343s]


Cleaning & indexing documents
-----------------------------

Similarly to the previous tutorials, we download, convert and index some
Game of Thrones articles to our DocumentStore

.. code:: ipython3

    # Let's first get some files that we want to use
    doc_dir = "data/article_txt_got"
    s3_url = "https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip"
    fetch_archive_from_http(url=s3_url, output_dir=doc_dir)
    
    # Convert files to dicts
    dicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)
    
    # Now, let's write the dicts containing documents to our DB.
    document_store.write_documents(dicts)


.. parsed-literal::

    07/03/2020 11:46:28 - INFO - haystack.indexing.utils -   Found data stored in `data/article_txt_got`. Delete this first if you really want to fetch new data.
    07/03/2020 11:46:28 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:0.300s]
    07/03/2020 11:46:28 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:0.209s]
    07/03/2020 11:46:28 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:0.124s]
    07/03/2020 11:46:29 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:0.101s]
    07/03/2020 11:46:29 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:0.125s]
    07/03/2020 11:46:29 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:0.096s]


Initalize Retriever, Reader, & Finder
-------------------------------------

Retriever
~~~~~~~~~

**Here:** We use a ``DensePassageRetriever``

**Alternatives:**

-  The ``ElasticsearchRetriever``\ with custom queries (e.g. boosting)
   and filters
-  Use ``EmbeddingRetriever`` to find candidate documents based on the
   similarity of embeddings (e.g. created via Sentence-BERT)
-  Use ``TfidfRetriever`` in combination with a SQL or InMemory Document
   store for simple prototyping and debugging

.. code:: ipython3

    from haystack.retriever.dense import DensePassageRetriever
    retriever = DensePassageRetriever(document_store=document_store, embedding_model="dpr-bert-base-nq",
                                      do_lower_case=True, use_gpu=True)
    
    # Important: 
    # Now that after we have the DPR initialized, we need to call update_embeddings() to iterate over all
    # previously indexed documents and update their embedding representation. 
    # While this can be a time consuming operation (depending on corpus size), it only needs to be done once. 
    # At query time, we only need to embed the query and compare it the existing doc embeddings which is very fast.
    document_store.update_embeddings(retriever)


.. parsed-literal::

    07/03/2020 11:46:29 - INFO - haystack.retriever.dpr_utils -   Loading saved model from models/dpr/checkpoint/retriever/single/nq/bert-base-encoder.cp
    07/03/2020 11:46:29 - INFO - haystack.retriever.dense -   Loaded encoder params:  {'do_lower_case': True, 'pretrained_model_cfg': 'bert-base-uncased', 'encoder_model_type': 'hf_bert', 'pretrained_file': None, 'projection_dim': 0, 'sequence_length': 256}
    07/03/2020 11:46:36 - INFO - haystack.retriever.dense -   Loading saved model state ...
    07/03/2020 11:46:36 - INFO - haystack.retriever.dense -   Loading saved model state ...
    07/03/2020 11:46:37 - INFO - elasticsearch -   GET http://localhost:9200/document/_search?scroll=5m&size=1000 [status:200 request:0.177s]
    07/03/2020 11:46:37 - INFO - elasticsearch -   GET http://localhost:9200/_search/scroll [status:200 request:0.053s]
    07/03/2020 11:46:37 - INFO - elasticsearch -   GET http://localhost:9200/_search/scroll [status:200 request:0.047s]
    07/03/2020 11:46:37 - INFO - elasticsearch -   GET http://localhost:9200/_search/scroll [status:200 request:0.003s]
    07/03/2020 11:46:37 - INFO - elasticsearch -   DELETE http://localhost:9200/_search/scroll [status:200 request:0.005s]
    07/03/2020 11:46:37 - INFO - haystack.database.elasticsearch -   Updating embeddings for 2811 docs ...
    07/03/2020 11:46:55 - INFO - haystack.retriever.dense -   Embedded 80 / 2811 texts
    07/03/2020 11:47:13 - INFO - haystack.retriever.dense -   Embedded 160 / 2811 texts
    07/03/2020 11:47:35 - INFO - haystack.retriever.dense -   Embedded 240 / 2811 texts
    07/03/2020 11:47:55 - INFO - haystack.retriever.dense -   Embedded 320 / 2811 texts
    07/03/2020 11:48:15 - INFO - haystack.retriever.dense -   Embedded 400 / 2811 texts
    07/03/2020 11:48:34 - INFO - haystack.retriever.dense -   Embedded 480 / 2811 texts
    07/03/2020 11:48:53 - INFO - haystack.retriever.dense -   Embedded 560 / 2811 texts
    07/03/2020 11:49:15 - INFO - haystack.retriever.dense -   Embedded 640 / 2811 texts
    07/03/2020 11:49:35 - INFO - haystack.retriever.dense -   Embedded 720 / 2811 texts
    07/03/2020 11:49:57 - INFO - haystack.retriever.dense -   Embedded 800 / 2811 texts
    07/03/2020 11:50:20 - INFO - haystack.retriever.dense -   Embedded 880 / 2811 texts
    07/03/2020 11:50:44 - INFO - haystack.retriever.dense -   Embedded 960 / 2811 texts
    07/03/2020 11:51:07 - INFO - haystack.retriever.dense -   Embedded 1040 / 2811 texts
    07/03/2020 11:51:29 - INFO - haystack.retriever.dense -   Embedded 1120 / 2811 texts
    07/03/2020 11:51:52 - INFO - haystack.retriever.dense -   Embedded 1200 / 2811 texts
    07/03/2020 11:52:14 - INFO - haystack.retriever.dense -   Embedded 1280 / 2811 texts
    07/03/2020 11:52:38 - INFO - haystack.retriever.dense -   Embedded 1360 / 2811 texts
    07/03/2020 11:53:00 - INFO - haystack.retriever.dense -   Embedded 1440 / 2811 texts
    07/03/2020 11:53:23 - INFO - haystack.retriever.dense -   Embedded 1520 / 2811 texts
    07/03/2020 11:53:48 - INFO - haystack.retriever.dense -   Embedded 1600 / 2811 texts
    07/03/2020 11:54:09 - INFO - haystack.retriever.dense -   Embedded 1680 / 2811 texts
    07/03/2020 11:54:33 - INFO - haystack.retriever.dense -   Embedded 1760 / 2811 texts
    07/03/2020 11:54:56 - INFO - haystack.retriever.dense -   Embedded 1840 / 2811 texts
    07/03/2020 11:55:18 - INFO - haystack.retriever.dense -   Embedded 1920 / 2811 texts
    07/03/2020 11:55:41 - INFO - haystack.retriever.dense -   Embedded 2000 / 2811 texts
    07/03/2020 11:56:08 - INFO - haystack.retriever.dense -   Embedded 2080 / 2811 texts
    07/03/2020 11:56:32 - INFO - haystack.retriever.dense -   Embedded 2160 / 2811 texts
    07/03/2020 11:56:54 - INFO - haystack.retriever.dense -   Embedded 2240 / 2811 texts
    07/03/2020 11:57:18 - INFO - haystack.retriever.dense -   Embedded 2320 / 2811 texts
    07/03/2020 11:57:40 - INFO - haystack.retriever.dense -   Embedded 2400 / 2811 texts
    07/03/2020 11:58:04 - INFO - haystack.retriever.dense -   Embedded 2480 / 2811 texts
    07/03/2020 11:58:39 - INFO - haystack.retriever.dense -   Embedded 2560 / 2811 texts
    07/03/2020 11:59:16 - INFO - haystack.retriever.dense -   Embedded 2640 / 2811 texts
    07/03/2020 11:59:53 - INFO - haystack.retriever.dense -   Embedded 2720 / 2811 texts
    07/03/2020 12:00:33 - INFO - haystack.retriever.dense -   Embedded 2800 / 2811 texts
    07/03/2020 12:00:42 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:2.577s]
    07/03/2020 12:00:44 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:2.085s]
    07/03/2020 12:00:46 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:1.918s]
    07/03/2020 12:00:49 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:1.933s]
    07/03/2020 12:00:51 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:1.562s]
    07/03/2020 12:00:52 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:1.150s]


Reader
~~~~~~

Similar to previous Tutorials we now initalize our reader.

Here we use a FARMReader with the *deepset/roberta-base-squad2* model
(see: https://huggingface.co/deepset/roberta-base-squad2)

FARMReader
^^^^^^^^^^

.. code:: ipython3

    # Load a  local model or any of the QA models on
    # Hugging Face's model hub (https://huggingface.co/models)
    
    reader = FARMReader(model_name_or_path="deepset/roberta-base-squad2", use_gpu=True)


.. parsed-literal::

    07/03/2020 12:00:52 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None
    07/03/2020 12:00:52 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...
    07/03/2020 12:00:59 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. 
    	 We guess it's an *ENGLISH* model ... 
    	 If not: Init the language model by supplying the 'language' param.
    07/03/2020 12:01:07 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {"loss_ignore_index": -1}
    /home/mp/miniconda3/envs/py37/lib/python3.7/site-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.
      category=FutureWarning,
    07/03/2020 12:01:11 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None
    07/03/2020 12:01:11 - INFO - farm.infer -   Got ya 7 parallel workers to do inference ...
    07/03/2020 12:01:11 - INFO - farm.infer -    0    0    0    0    0    0    0 
    07/03/2020 12:01:11 - INFO - farm.infer -   /w\  /w\  /w\  /w\  /w\  /w\  /w\
    07/03/2020 12:01:11 - INFO - farm.infer -   /'\  / \  /'\  /'\  / \  / \  /'\
    07/03/2020 12:01:11 - INFO - farm.infer -               


Finder
~~~~~~

The Finder sticks together reader and retriever in a pipeline to answer
our actual questions.

.. code:: ipython3

    finder = Finder(reader, retriever)

Voilà! Ask a question!
----------------------

.. code:: ipython3

    # You can configure how many candidates the reader and retriever shall return
    # The higher top_k_retriever, the better (but also the slower) your answers. 
    prediction = finder.get_answers(question="Who created the Dothraki vocabulary?", top_k_retriever=10, top_k_reader=5)
    
    #prediction = finder.get_answers(question="Who is the father of Arya Stark?", top_k_retriever=10, top_k_reader=5)
    #prediction = finder.get_answers(question="Who is the sister of Sansa?", top_k_retriever=10, top_k_reader=5)


.. parsed-literal::

    07/03/2020 12:07:16 - INFO - elasticsearch -   GET http://localhost:9200/document/_search [status:200 request:0.018s]
    07/03/2020 12:07:16 - INFO - haystack.finder -   Reader is looking for detailed answer in 11362 chars ...
    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.09 Batches/s]
    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.05 Batches/s]
    Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.76s/ Batches]
    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.50 Batches/s]
    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.93 Batches/s]
    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.15 Batches/s]
    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.64 Batches/s]
    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.93 Batches/s]
    Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.60s/ Batches]
    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.10 Batches/s]


.. code:: ipython3

    print_answers(prediction, details="minimal")


.. parsed-literal::

    [   {   'answer': 'David J. Peterson',
            'context': "ge for ''Game of Thrones''\n"
                       'The Dothraki vocabulary was created by David J. Peterson '
                       'well in advance of the adaptation. HBO hired the Language '
                       'Creation'},
        {   'answer': 'David J. Peterson',
            'context': '\n'
                       '===Creation===\n'
                       'David J. Peterson, creator of the spoken Valyrian '
                       "languages for ''Game of Thrones''\n"
                       'To create the Dothraki and Valyrian languages to b'},
        {   'answer': 'David J. Peterson',
            'context': 'orld. The language was developed for the TV series by the '
                       'linguist David J. Peterson, working off the Dothraki words '
                       "and phrases in Martin's novels.\n"
                       ','},
        {   'answer': 'Peterson',
            'context': "e does not exist in the fictional world of ''A Song of Ice "
                       "and Fire'', Peterson chose to treat the similarity as "
                       "coincidental and made ''dracarys'' an"},
        {   'answer': 'Peterson',
            'context': 'ystem. Another word, \'\'trēsy\'\', meaning "son", was '
                       "coined in honour of Peterson's 3000th Twitter follower.\n"
                       'Peterson did not create a High Valyrian wri'}]

